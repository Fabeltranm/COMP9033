{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9033 - Data Analytics Lab 06: Linear regression\n",
    "## Introduction\n",
    "\n",
    "This week's lab focuses on data modelling using linear regression. At the end of the lab, you should be able to use `scikit-learn` to:\n",
    "\n",
    "- Create a linear regression model using the least squares technique.\n",
    "- Use the model to predict new values.\n",
    "- Measure the accuracy of the model.\n",
    "- Engineer new features to optimise model performance.\n",
    "\n",
    "### Getting started\n",
    "\n",
    "Let's start by importing the packages we need. This week, we're going to use the `linear_model` subpackage from `scikit-learn` to build linear regression models using the least squares technique. We're also going to need the `dummy` subpackage to create a baseline regression model, to which we can compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the data. This week, we're going to load the [Auto MPG](https://archive.ics.uci.edu/ml/datasets/Auto+MPG) data set, which is available online at the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). The dataset is in [fixed width format](https://www.ibm.com/support/knowledgecenter/SSFTDH_8.0.0/com.ibm.wbpm.wid.integ.doc/topics/rfixwidth.html), but fortunately this is supported out of the box by `pandas`' [`read_fwf`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_fwf.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "\n",
    "df = pd.read_fwf(url, header=None, names=['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "                                          'acceleration', 'model year', 'origin', 'car name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "According to its [documentation](https://archive.ics.uci.edu/ml/datasets/Auto+MPG), the Auto MPG dataset consists of eight explantory variables (i.e. features), each describing a single car model, which are related to the given target variable: the number of miles per gallon (MPG) of fuel of the given car. The following attribute information is given:\n",
    "\n",
    "1. mpg: continuous\n",
    "2. cylinders: multi-valued discrete\n",
    "3. displacement: continuous\n",
    "4. horsepower: continuous\n",
    "5. weight: continuous\n",
    "6. acceleration: continuous\n",
    "7. model year: multi-valued discrete\n",
    "8. origin: multi-valued discrete\n",
    "9. car name: string (unique for each instance)\n",
    "\n",
    "Let's start by taking a quick peek at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the car name is unique for each instance (according to the [dataset documentation](https://archive.ics.uci.edu/ml/datasets/Auto+MPG)), it cannot be used to predict the MPG by itself so let's drop it as a feature and use it as the index instead:\n",
    "\n",
    "> **Note:** It seems plausible that MPG efficiency might vary from manufacturer to manufacturer, so we could generate a new feature by converting the car names into manufacturer names, but for simplicity lets just drop them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.set_index('car name')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [documentation](https://archive.ics.uci.edu/ml/datasets/Auto+MPG), the horsepower column contains a small number of missing values, each of which is denoted by the string `'?'`. Again, for simplicity, let's just drop these from the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['horsepower'] != '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, pandas is smart enough to recognise that a column is numeric and will convert it to the appropriate data type automatically. However, in this case, because there were strings present initially, the value type of the horsepower column isn't numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can correct this by converting the column values numbers manually, using `pandas`' [`to_numeric`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['horsepower'] = pd.to_numeric(df['horsepower'])\n",
    "\n",
    "# Check the data types again\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the data type of the horsepower column is now `float64`, i.e. a 64 bit floating point value.\n",
    "\n",
    "According to the [documentation](https://archive.ics.uci.edu/ml/datasets/Auto+MPG), the origin variable is categoric (i.e. origin = 1 is not \"less than\" origin = 2) and so we should encode it via one hot encoding so that our model can make sense of it. This is easy with `pandas`: all we need to do is use the [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) method, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['origin'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, one hot encoding converts the origin column into three separate binary columns, each representing the presence or absence of the given category.\n",
    "\n",
    "Next, let's take a look at the distribution of the variables in the data frame. We can start by computing some descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a matrix of pairwise Pearson correlation values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a scatter plot matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df, s=50, hist_kwds={'bins': 10}, figsize=(16, 16));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above information, we can conclude the following:\n",
    "\n",
    "- Based on a quick visual inspection, there don't appear to be significant numbers of outliers in the data set. (We could make boxplots for each variable - but let's save time and skip it here.)\n",
    "- Most of the explanatory variables appear to have a non-linear relationship with the target.\n",
    "- There is a high degree of correlation ($r > 0.9$) between cylinders and displacement and, also, between weight and displacement.\n",
    "- The following variables appear to be left-skewed: mpg, displacement, horsepower, weight.\n",
    "- The acceleration variable appears to be normally distributed.\n",
    "- The model year follows a rough uniform distributed.\n",
    "- The cylinders and origin variables have few unique values.\n",
    "\n",
    "For now, we'll just note this information, but we'll come back to it later when improving our model.\n",
    "\n",
    "## Data Modelling\n",
    "### Dummy model\n",
    "\n",
    "Let's start our analysis by building a dummy regression model that makes very naive (often incorrect) predictions about the target variable. This is a good first step as it gives us a benchmark to compare our later models to.\n",
    "\n",
    "Creating a dummy regression model with `scikit-learn` is easy: first, we create an instance of [`DummyRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html#sklearn.dummy.DummyRegressor), and then we evaluate its performance on the data using cross validation, just like last week.\n",
    "\n",
    "> **Note:** Our dummy model has no hyperparameters, so we don't need to do an inner cross validation or grid search - just the outer cross validation to estimate the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.drop('mpg', axis='columns')  # X = features\n",
    "y = df['mpg']                       # y = prediction target\n",
    "\n",
    "model = DummyRegressor()  # Predicts the target as the average of the features\n",
    "\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=0)  # 5 fold cross validation\n",
    "y_pred = cross_val_predict(model, X, y, cv=outer_cv)\n",
    "\n",
    "print('Mean absolute error: %f' % mean_absolute_error(y, y_pred))\n",
    "print('Standard deviation of the error: %f' % (y - y_pred).std())\n",
    "\n",
    "ax = (y - y_pred).hist()\n",
    "ax.set(\n",
    "    title='Distribution of errors for the dummy model',\n",
    "    xlabel='Error'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy model predicts the MPG with an average error of approximately $\\pm 6.57$ (although, as can be seen from the distribution of errors the spread is much larger than this). Let's see if we can do better with a linear regression model.\n",
    "\n",
    "### Linear regression model\n",
    "\n",
    "`scikit-learn` supports linear regression via its [`linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) subpackage. This subpackage supports least squares regression, lasso regression and ridge regression, as well as many other varieties. Let's use least squares to build our model. We can do this using the [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.drop('mpg', axis='columns')\n",
    "y = df['mpg']\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "y_pred = cross_val_predict(model, X, y, cv=outer_cv)\n",
    "\n",
    "print('Mean absolute error: %f' % mean_absolute_error(y, y_pred))\n",
    "print('Standard deviation of the error: %f' % (y - y_pred).std())\n",
    "\n",
    "ax = (y - y_pred).hist()\n",
    "ax.set(\n",
    "    title='Distribution of errors for the linear regression model',\n",
    "    xlabel='Error'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our linear regression model predicts the MPG with an average error of approximately $\\pm 2.59$ and a significantly smaller standard deviation too - this is a big improvement over the dummy model!\n",
    "\n",
    "But we can do better! Earlier, we noted that several of the features had non-linear relationships with the target variable - if we could transform these variables, we might be able to make this relationship more linear. Let's consider the displacement, horsepower and weight variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df[['mpg', 'displacement', 'horsepower', 'weight']], s=50, figsize=(9, 9));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between the target and these predictors appears to be an exponentially decreasing one: as the predictors increase in value, there is an exponential decrease in the target value. Log-transforming the variables should help to remove this effect (logarithms are the inverse mathematical operation to exponentials):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['displacement'] = df['displacement'].map(np.log)\n",
    "df['horsepower'] = df['horsepower'].map(np.log)\n",
    "df['weight'] = df['weight'].map(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the relationship between the predictors and the target is much more linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df[['mpg', 'displacement', 'horsepower', 'weight']], s=50, figsize=(9, 9));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the analysis a second time and see the effect this has had:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.drop('mpg', axis='columns')\n",
    "y = df['mpg']\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "y_pred = cross_val_predict(model, X, y, cv=outer_cv)\n",
    "\n",
    "print('Mean absolute error: %f' % mean_absolute_error(y, y_pred))\n",
    "print('Standard deviation of the error: %f' % (y - y_pred).std())\n",
    "\n",
    "ax = (y - y_pred).hist()\n",
    "ax.set(\n",
    "    title='Distribution of errors for the linear regression model with transformed features',\n",
    "    xlabel='Error'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the average error has now decreased to $\\pm 2.33$ and the standard deviation of the error to 3.12. Further reductions in error might be achieved by experimenting with feature selection, given the high degree of correlation between some of the predictors, or with a more sophisticated model, such as ridge regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
