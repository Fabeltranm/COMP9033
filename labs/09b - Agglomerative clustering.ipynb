{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09b: Agglomerative clustering\n",
    "## Introduction\n",
    "\n",
    "This lab focuses on agglomerative clustering for determining the similarity of documents. At the end of the lab, you should be able to:\n",
    "\n",
    "- Create an agglomerative clustering model.\n",
    "- Plot a dendrogram of the cluster levels.\n",
    "\n",
    "### Getting started\n",
    "\n",
    "Let's start by importing the packages we'll need. As usual, we'll import `pandas` for exploratory analysis, but this week we're also going to use the `cluster` subpackage from `scikit-learn` to create agglomerative clustering models and the standard Python package `urllib2` to download documents from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import urllib2\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we're going to cluster documents by the similarity of their text content. For this, we'll need to download some documents to cluster. The following dictionary maps the names of various texts to their corresponding URLs at [Project Gutenberg](https://www.gutenberg.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = {\n",
    "    'The Iliad - Homer': 'https://www.gutenberg.org/cache/epub/1727/pg1727.txt',\n",
    "    'The Odyssey - Homer': 'https://www.gutenberg.org/cache/epub/1727/pg1727.txt',\n",
    "    'Romeo and Juliet - William Shakespeare': 'https://www.gutenberg.org/cache/epub/1112/pg1112.txt',\n",
    "    'Hamlet - William Shakespeare': 'https://www.gutenberg.org/files/1524/1524-0.txt',\n",
    "    'Adventures of Huckleberry Finn - Mark Twain': 'https://www.gutenberg.org/files/76/76-0.txt',\n",
    "    'The Adventures of Tom Sawyer - Mark Twain': 'https://www.gutenberg.org/files/74/74-0.txt',\n",
    "    'A Tale of Two Cities - Charles Dickens': 'https://www.gutenberg.org/files/98/98-0.txt',\n",
    "    'Great Expectations - Charles Dickens': 'https://www.gutenberg.org/files/1400/1400-0.txt',\n",
    "    'Oliver Twist - Charles Dickens': 'https://www.gutenberg.org/cache/epub/730/pg730.txt',\n",
    "    'The Adventures of Sherlock Holmes - Arthur Conan Doyle': 'https://www.gutenberg.org/cache/epub/1661/pg1661.txt'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to download the texts located at the URLs. We can do this using Python's [`urllib2`](https://docs.python.org/2/library/urllib2.html) package, which is part of the standard Python library. The following code will download the content of each URL and store it in the `documents` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = {}\n",
    "for name, url in urls.items():\n",
    "    response = urllib2.urlopen(url)\n",
    "    document = response.read()\n",
    "    documents[name] = document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a `pandas` data frame to represent our document data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([documents[name] for name in sorted(documents)], index=sorted(documents), columns=['text'])\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data modelling\n",
    "\n",
    "Let's build an agglomerative clustering model of the document data. As with $K$-means clustering, `scikit-learn` supports agglomerative clustering functionality via the [`cluster`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster) subpackage. We can use the [`AgglomerativeClustering`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) class to build our model.\n",
    "\n",
    "As with other `scikit-learn` estimators, `AgglomerativeClustering` accepts a number of different hyperparameters. We can get a list of these modelling parameters using the `get_params` method of the estimator (this works on any `scikit-learn` estimator), like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AgglomerativeClustering().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a more detailed description of each parameter in the `scikit-learn` [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html). \n",
    "\n",
    "As our data is in text format, we'll need to convert it into a numerical representation so that it can be understood by the clustering algorithm. One way to do this is by converting the document texts into vectors of TF-IDF scores, just as we did when building the spam classifier. This way, the clustering algorithm will identify documents with similar TF-IDF score vectors. This should result in clusters containing documents with similar text content, because if two documents have similar TF-IDF vectors, then they must contain the same words, occurring with the same frequencies.\n",
    "\n",
    "> **Note:** Comparing TF-IDF score vectors is one - but not the only - way to determine whether documents have similar content.\n",
    "\n",
    "As with the spam classification example, we can use a pipeline to connect the `TfidfVectorizer` to the `AgglomerativeClustering` algorithm. Because of a snag in the way `scikit-learn` is coded, the `AgglomerativeClustering` class only accepts dense matrices as inputs and, unfortunately, `TfidfVectorizer` produces [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix) output. However, this is easily recified by inserting a [`FunctionTransformer`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer) (essentially, a custom function) between the two that converts the sparse input to dense input.\n",
    "\n",
    "The code specifying the pipeline and fitting the data is shown below. Note that, as with $K$-means clustering, agglomerative clustering is an unsupervised learning algorithm, and so we don't need to specify a target variable ($y$) when fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "\n",
    "# Construct a pipeline: TF-IDF -> Sparse to Dense -> Clustering\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(stop_words='english'),\n",
    "    FunctionTransformer(lambda x: x.todense(), accept_sparse=True),\n",
    "    AgglomerativeClustering(linkage='average')  # Use average linkage\n",
    ")\n",
    "\n",
    "pipeline = pipeline.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've fitted the data to the pipeline, we can extract the fitted agglomerative clustering model to see what clusters were formed. To extract the model, we can use the [`named_steps`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) attribute of the pipeline, which is a dictionary mapping the names (in lowercase) of each stage in the pipeline to the corresponding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, our clustering model is stored under the key `'agglomerativeclustering'`, and so we can extract it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = pipeline.named_steps['agglomerativeclustering']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, `scikit-learn` does not support plotting dendrograms out of the box. However, the authors have provided the following code snippet for anyone who wants to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Original source: https://github.com/scikit-learn/scikit-learn/blob/70cf4a676caa2d2dad2e3f6e4478d64bcb0506f7/examples/cluster/plot_hierarchical_clustering_dendrogram.py\n",
    "import numpy as np\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0] + 2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can call the `plot_dendrogram` function to plot a dendrogram of our model, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_dendrogram(model, labels=X.index, orientation='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, clustering by TF-IDF score vectors results in texts written by the same author or at roughly the same point in time being grouped together. This makes sense when you consider that TF-IDF scores represent the frequency of use of certain terms, which may be indicative of an individual author's style or the style of writing at a certain point in history."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
